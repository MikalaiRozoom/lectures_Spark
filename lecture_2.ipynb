{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Задание-схемы-DataFrame\" data-toc-modified-id=\"Задание-схемы-DataFrame-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Задание схемы DataFrame</a></span></li><li><span><a href=\"#Задачи-с-пожарными\" data-toc-modified-id=\"Задачи-с-пожарными-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Задачи с пожарными</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-were-all-the-different-types-of-fire-calls-in-2018\" data-toc-modified-id=\"What-were-all-the-different-types-of-fire-calls-in-2018-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>What were all the different types of fire calls in 2018</a></span></li><li><span><a href=\"#What-months-within-the-year-2018-saw-the-highest-number-of-fire-calls?\" data-toc-modified-id=\"What-months-within-the-year-2018-saw-the-highest-number-of-fire-calls?-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>What months within the year 2018 saw the highest number of fire calls?</a></span></li><li><span><a href=\"#Which-neighborhood-in-San-Francisco-generated-the-most-fire-calls-in-2018?\" data-toc-modified-id=\"Which-neighborhood-in-San-Francisco-generated-the-most-fire-calls-in-2018?-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Which neighborhood in San Francisco generated the most fire calls in 2018?</a></span></li><li><span><a href=\"#Which-neighborhoods-had-the-worst-response-times-to-fire-calls-in-2018?\" data-toc-modified-id=\"Which-neighborhoods-had-the-worst-response-times-to-fire-calls-in-2018?-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Which neighborhoods had the worst response times to fire calls in 2018?</a></span></li><li><span><a href=\"#Which-week-in-the-year-in-2018-had-the-most-fire-calls?\" data-toc-modified-id=\"Which-week-in-the-year-in-2018-had-the-most-fire-calls?-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Which week in the year in 2018 had the most fire calls?</a></span></li></ul></li><li><span><a href=\"#Create-UDF\" data-toc-modified-id=\"Create-UDF-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create UDF</a></span></li><li><span><a href=\"#Join-on-Single-Field\" data-toc-modified-id=\"Join-on-Single-Field-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Join on Single Field</a></span></li><li><span><a href=\"#Join-on-Multiple-Fields\" data-toc-modified-id=\"Join-on-Multiple-Fields-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Join on Multiple Fields</a></span></li><li><span><a href=\"#spark.sql-directly\" data-toc-modified-id=\"spark.sql-directly-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>spark.sql directly</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание схемы DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "\n",
    "# Create our static data\n",
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    "]\n",
    "\n",
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data=data, schema=schema)\n",
    "# Show the DataFrame; it should reflect our table above\n",
    "blogs_df.show()\n",
    "# Print the schema used by Spark to process the DataFrame\n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "blogs_df\\\n",
    ".selectExpr(\"*\" , \"concat(first, ' ', last) full_name\")\\\n",
    ".agg(sum(col(\"Hits\")).alias(\"Hits\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "events_df = spark.table(\"market.events\")\n",
    "grouped = events_df.groupBy(\"brand\").agg(f.count(\"*\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "blogs_df\\\n",
    ".where(\"hits > 10000\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи с пожарными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md \n",
    "\n",
    "What were all the different types of fire calls in 2018?\n",
    "What months within the year 2018 saw the highest number of fire calls?\n",
    "Which neighborhood in San Francisco generated the most fire calls in 2018?\n",
    "Which neighborhoods had the worst response times to fire calls in 2018?\n",
    "Which week in the year in 2018 had the most fire calls?\n",
    "Is there a correlation between neighborhood, zip code, and number of fire calls?\n",
    "How can we use Parquet files or SQL tables to store this data and read it back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "curl http://37.139.43.86/sf-fire-calls > sf-fire-calls.csv\n",
    "\n",
    "hdfs dfs -put sf-fire-calls.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sparkLocal.pyspark\n",
    "\n",
    "spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"data/sf-fire-calls.csv\") \\\n",
    ".write.parquet(\"data/sf-fire-calls.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "fireDF = spark.read.option(\"header\", True).load(\"data/sf-fire-calls.parquet\")\n",
    "fireDF.printSchema()\n",
    "fireDF.show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What were all the different types of fire calls in 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "fireDF \\\n",
    ".withColumn(\"CallDate\", \n",
    "    f.to_timestamp(f.col(\"CallDate\"), 'dd/MM/yyyy'))\\\n",
    ".withColumn(\"CallYear\", f.year(\"CallDate\"))\\\n",
    ".where(\"CallYear = '2018'\") \\\n",
    ".select(\"CallType\")\\\n",
    ".distinct()\\\n",
    ".show(1000, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What months within the year 2018 saw the highest number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "fireDF\\\n",
    ".withColumn(\"CallDate\", f.to_timestamp(f.col(\"CallDate\"), 'dd/MM/yyyy'))\\\n",
    ".withColumn(\"CallMonth\", f.month(\"CallDate\"))\\\n",
    ".withColumn(\"CallYear\", f.year(\"CallDate\"))\\\n",
    ".where(\"CallYear = '2018'\") \\\n",
    ".groupBy(\"CallMonth\")\\\n",
    ".agg(\n",
    "    f.countDistinct(\"callNumber\").alias(\"call_count\")\n",
    "    )\\\n",
    ".orderBy(\"call_count\", ascending=False)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which neighborhood in San Francisco generated the most fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "(fireDF\n",
    ".groupBy(\"Neighborhood\")\n",
    ".agg(countDistinct(\"callNumber\").alias(\"call_count\"))\n",
    ".orderBy(\"call_count\", ascending=False)\n",
    ".show(1000, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which neighborhoods had the worst response times to fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "fireDF\\\n",
    ".where(\"lower(CallType) like '%fire%'\")\\\n",
    ".select(\"Neighborhood\", \"delay\", \"callType\")\\\n",
    ".orderBy(\"delay\", ascending=False)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which week in the year in 2018 had the most fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "fireDF\\\n",
    ".where(\"lower(CallType) like '%fire%'\")\\\n",
    ".withColumn(\"CallDate\", f.to_timestamp(f.col(\"CallDate\"), 'dd/MM/yyyy'))\\\n",
    ".withColumn(\"CallWeek\", f.weekofyear(\"CallDate\"))\\\n",
    ".withColumn(\"CallYear\", f.year(\"CallDate\"))\\\n",
    ".where(\"CallYear = '2018'\") \\\n",
    ".groupBy(\"CallYear\", \"CallWeek\")\\\n",
    ".agg(\n",
    "    f.countDistinct(\"callNumber\").alias(\"call_count\")\n",
    "    )\\\n",
    ".orderBy(\"call_count\", ascending=False)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    (1, \"andy\", 20, \"USA\"), \n",
    "    (2, \"jeff\", 23, \"China\"), \n",
    "    (3, \"james\", 18, \"USA\")]) \\\n",
    ".toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "\n",
    "# Create udf create python lambda\n",
    "from pyspark.sql.functions import udf\n",
    "udf1 = udf(lambda e: e.upper())\n",
    "df2 = df1.select(udf1(df1[\"name\"]))\n",
    "df2.show()\n",
    "\n",
    "# UDF could also be used in filter, in this case the return type must be Boolean\n",
    "# We can also use annotation to create udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def udf2(e):\n",
    "    if e >= 20:\n",
    "        return True;\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df3 = df1.filter(udf2(df1[\"age\"]))\n",
    "df3.show()\n",
    "\n",
    "# UDF could also accept more than 1 argument.\n",
    "udf3 = udf(lambda e1, e2: e1 + \"_\" + e2)\n",
    "df4 = df1.select(udf3(df1[\"name\"], df1[\"country\"]).alias(\"name_country\"))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join on Single Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    (1, \"andy\", 20, 1), \n",
    "    (2, \"jeff\", 23, 2), \n",
    "    (3, \"james\", 18, 3)])\n",
    "    .toDF(\"id\", \"name\", \"age\", \"c_id\")\n",
    "    \n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"USA\"), (2, \"China\")]).toDF(\"c_id\", \"c_name\")\n",
    "df2.show()\n",
    "\n",
    "# You can just specify the key name if join on the same key\n",
    "df3 = df1.join(df2, on=[\"c_id\"], how=\"inner\")\n",
    "df3.show()\n",
    "\n",
    "# Or you can specify the join condition expclitly in case the key is different between tables\n",
    "df4 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"] )\n",
    "df4.show()\n",
    "\n",
    "# You can specify the join type afte the join condition, by default it is inner join\n",
    "df5 = df1.join(df2, df1[\"c_id\"] == df2[\"c_id\"], \"left_outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join on Multiple Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([(\"andy\", 20, 1, 1), (\"jeff\", 23, 1, 2), (\"james\", 12, 2, 2)]).toDF(\"name\", \"age\", \"key_1\", \"key_2\")\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame([(1, 1, \"USA\"), (2, 2, \"China\")]).toDF(\"key_1\", \"key_2\", \"country\")\n",
    "df2.show()\n",
    "\n",
    "# Join on 2 fields: key_1, key_2\n",
    "\n",
    "# You can pass a list of field name if the join field names are the same in both tables\n",
    "df3 = df1.join(df2, [\"key_1\", \"key_2\"])\n",
    "df3.show()\n",
    "\n",
    "# Or you can specify the join condition expclitly in case when the join fields name is differetnt in the two tables\n",
    "df4 = df1.join(df2, (df1[\"key_1\"] == df2[\"key_1\"]) & (df1[\"key_2\"] == df2[\"key_2\"]))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark.sql directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "        (1, \"andy\", 20, \"USA\"), \n",
    "        (2, \"jeff\", 23, \"China\"), \n",
    "        (3, \"james\", 18, \"USA\")]) \\\n",
    "    .toDF(\"id\", \"name\", \"age\", \"country\")\n",
    "    \n",
    "df1.createOrReplaceTempView(\"people\")\n",
    "\n",
    "df2 = spark.\n",
    "(\"select name, age from people\")\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1820,
    "start_time": "2021-12-02T23:15:57.972Z"
   },
   {
    "duration": 113,
    "start_time": "2021-12-02T23:15:59.794Z"
   },
   {
    "duration": 16,
    "start_time": "2021-12-02T23:16:01.802Z"
   },
   {
    "duration": 9,
    "start_time": "2021-12-02T23:16:04.022Z"
   },
   {
    "duration": 8,
    "start_time": "2021-12-02T23:16:06.131Z"
   },
   {
    "duration": 15,
    "start_time": "2021-12-02T23:16:07.242Z"
   },
   {
    "duration": 398,
    "start_time": "2021-12-02T23:16:08.129Z"
   },
   {
    "duration": 10,
    "start_time": "2021-12-02T23:16:17.583Z"
   },
   {
    "duration": 3,
    "start_time": "2021-12-02T23:16:18.198Z"
   },
   {
    "duration": 9,
    "start_time": "2021-12-02T23:16:18.793Z"
   },
   {
    "duration": 11,
    "start_time": "2021-12-02T23:16:21.930Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:16:22.378Z"
   },
   {
    "duration": 260,
    "start_time": "2021-12-02T23:16:30.000Z"
   },
   {
    "duration": 5,
    "start_time": "2021-12-02T23:16:34.629Z"
   },
   {
    "duration": 2934,
    "start_time": "2021-12-02T23:16:35.275Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:16:41.192Z"
   },
   {
    "duration": 397,
    "start_time": "2021-12-02T23:16:41.783Z"
   },
   {
    "duration": 245,
    "start_time": "2021-12-02T23:16:45.015Z"
   },
   {
    "duration": 4,
    "start_time": "2021-12-02T23:17:25.956Z"
   },
   {
    "duration": 11,
    "start_time": "2021-12-02T23:17:26.132Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:17:32.388Z"
   },
   {
    "duration": 698,
    "start_time": "2021-12-02T23:17:34.958Z"
   },
   {
    "duration": 1414,
    "start_time": "2021-12-02T23:17:38.152Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:17:41.927Z"
   },
   {
    "duration": 1572,
    "start_time": "2021-12-02T23:17:43.848Z"
   },
   {
    "duration": 1156,
    "start_time": "2021-12-02T23:17:48.999Z"
   },
   {
    "duration": 23205,
    "start_time": "2021-12-07T18:42:27.953Z"
   },
   {
    "duration": 2694,
    "start_time": "2021-12-07T18:45:34.103Z"
   },
   {
    "duration": 302,
    "start_time": "2021-12-07T19:15:48.509Z"
   },
   {
    "duration": 21043,
    "start_time": "2021-12-07T19:15:48.813Z"
   },
   {
    "duration": 3,
    "start_time": "2021-12-07T19:49:43.957Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T19:57:11.654Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T20:07:12.050Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T20:14:58.374Z"
   },
   {
    "duration": 86,
    "start_time": "2021-12-10T10:58:16.325Z"
   },
   {
    "duration": 4,
    "start_time": "2021-12-10T11:14:07.509Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "801px",
    "left": "24px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
