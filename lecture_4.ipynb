{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Skew-data\" data-toc-modified-id=\"Skew-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Skew data</a></span></li><li><span><a href=\"#Добавление-соли\" data-toc-modified-id=\"Добавление-соли-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Добавление соли</a></span></li><li><span><a href=\"#Кеширование\" data-toc-modified-id=\"Кеширование-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Кеширование</a></span></li><li><span><a href=\"#Планы-выполнения-задач\" data-toc-modified-id=\"Планы-выполнения-задач-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Планы выполнения задач</a></span></li><li><span><a href=\"#Оптимизация-соединений-и-группировок\" data-toc-modified-id=\"Оптимизация-соединений-и-группировок-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Оптимизация соединений и группировок</a></span><ul class=\"toc-item\"><li><span><a href=\"#BroadcastHashJoin\" data-toc-modified-id=\"BroadcastHashJoin-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>BroadcastHashJoin</a></span></li><li><span><a href=\"#SortMergeJoin\" data-toc-modified-id=\"SortMergeJoin-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>SortMergeJoin</a></span></li><li><span><a href=\"#BroadcastNestedLoopJoin\" data-toc-modified-id=\"BroadcastNestedLoopJoin-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>BroadcastNestedLoopJoin</a></span></li><li><span><a href=\"#CartesianProduct\" data-toc-modified-id=\"CartesianProduct-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>CartesianProduct</a></span></li></ul></li><li><span><a href=\"#Снижение-количества-shuffle\" data-toc-modified-id=\"Снижение-количества-shuffle-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Снижение количества shuffle</a></span></li><li><span><a href=\"#Оптимизатор-запросов-Catalyst\" data-toc-modified-id=\"Оптимизатор-запросов-Catalyst-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Оптимизатор запросов Catalyst</a></span><ul class=\"toc-item\"><li><span><a href=\"#Column-projection\" data-toc-modified-id=\"Column-projection-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Column projection</a></span></li><li><span><a href=\"#Partition-pruning\" data-toc-modified-id=\"Partition-pruning-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Partition pruning</a></span></li><li><span><a href=\"#Predicate-pushdown\" data-toc-modified-id=\"Predicate-pushdown-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Predicate pushdown</a></span></li><li><span><a href=\"#Simplify-casts\" data-toc-modified-id=\"Simplify-casts-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Simplify casts</a></span></li><li><span><a href=\"#Constant-folding\" data-toc-modified-id=\"Constant-folding-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Constant folding</a></span></li><li><span><a href=\"#Combine-filters\" data-toc-modified-id=\"Combine-filters-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Combine filters</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r%sh\n",
    "\n",
    "hdfs dfs -ls -h /apps/hive/warehouse/market.db/events/date=2019-11-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conf\n",
    "\n",
    "spark.executor.memory=512mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.table(\"hw_4.events_full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"hw_4.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.006)\n",
    ".repartition(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw_4.sample_big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"hw_4.events_full\")\n",
    ".select(\"event_id\")\n",
    ".sample(0.03)\n",
    ".repartition(1)\n",
    ".write.mode(\"overwrite\")\n",
    ".saveAsTable(\"hw_4.sample_very_big\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "sample_very_big_df = spark.table(\"hw_4.sample_very_big\")\n",
    "sample_big_df = spark.table(\"hw_4.sample_big\")\n",
    "\n",
    "sample_very_big_df.join(sample_big_df, \"event_id\", \"leftsemi\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.table(\"hw_4.sample_very_big\") \\\n",
    ".write.mode(\"overwrite\").bucketBy(10, \"event_id\") \\\n",
    ".saveAsTable(\"hw_4.sample_very_big_bucket\")\n",
    "\n",
    "\n",
    "spark.table(\"hw_4.sample_big\") \\\n",
    ".write.mode(\"overwrite\").bucketBy(10, \"event_id\") \\\n",
    ".saveAsTable(\"hw_4.sample_big_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -ls -h /apps/hive/warehouse/hw_4.db/sample_big\n",
    "\n",
    "hdfs dfs -ls -h /apps/hive/warehouse/hw_4.db/sample_very_big\n",
    "\n",
    "hdfs dfs -ls -h /apps/hive/warehouse/hw_4.db/sample_very_big_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "sample_very_big_df = spark.table(\"hw_4.sample_very_big_bucket\")\n",
    "sample_big_df = spark.table(\"hw_4.sample_big_bucket\")\n",
    "\n",
    "sample_very_big_df.join(sample_big_df, \"event_id\", \"leftsemi\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skew data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "skew_column = when(col(\"id\") < 900, lit(0)).otherwise(lit(1)).alias(\"skew_column\")\n",
    "skewed_df = spark.range(1000).withColumn(\"skew\", skew_column).repartition(10, col(\"skew\"))\n",
    "skewed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "def print_parts(df):\n",
    "    ret = df.rdd.mapPartitions(lambda x: [len(list(x))]).collect()\n",
    "    print(ret)\n",
    "    \n",
    "print_parts(skewed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "# здесь мы передаем только новое количество партиций и Spark выполнит RoundRobinPartitioning\n",
    "\n",
    "balanced_df = skewed_df.repartition(11)\n",
    "print_parts(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# здесь мы добавляем к числу партиций колонку, по которой необходимо сделать репартиционирование,\n",
    "# поэтому Spark выполнит HashPartitioning\n",
    "\n",
    "balanced_df = skewed_df.repartition(20, col(\"id\"))\n",
    "print_parts(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавление соли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "curl https://github.com/datasets/airport-codes/raw/master/data/airport-codes.csv  > airport-codes.csv\n",
    "hdfs dfs -put airport-codes.csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sparkLocal.pyspark\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df = spark.read.format(\"csv\").options(header=True, inferSchema=True).load(\"data/airport-codes.csv\")\n",
    "\n",
    "df.groupBy(f.col(\"type\")).count().orderBy(f.col(\"count\").desc()).show(30, False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import collect_list, col\n",
    "\n",
    "skew_grouped = df.groupBy(col(\"type\")).agg(collect_list(col(\"ident\")).alias(\"ids\"))\n",
    "skew_grouped.show(20, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при вычислении агрегата происходит неявный HashPartitioning по ключу (ключам) агрегата, то при выполнении определенных условий происходит нехватка памяти на воркере, которую нельзя исправить, не изменив подход к построению агрегата.\n",
    "\n",
    "Один из вариантов устранение - соление ключей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "salt = expr(\"\"\"pmod(round(rand() * 100, 0), 10)\"\"\").cast(\"integer\")\n",
    "salted = df.withColumn(\"salt\", salt)\n",
    "salted.select(col(\"type\"), col(\"ident\"), col(\"salt\")).sample(0.1).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам существенно снизить объем данных в каждой партиции (30к vs 3к):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "salted.groupBy(col(\"type\"), col(\"salt\")).count().orderBy(col(\"count\").desc()).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это позволяет нам посчитать требуемый агрегат более оптимальным путем, не смотря на появление второго агрегата:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salted \\\n",
    "    .groupBy(col(\"type\"), col(\"salt\")).agg(collect_list(col(\"ident\")).alias(\"ids\")) \\\n",
    "    .groupBy(col(\"type\")).agg(collect_list(col(\"ids\")).alias(\"ids\")) \\\n",
    "    .show(20, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кеширование\n",
    "По умолчанию при применении каждого действия Spark пересчитывает весь граф, что может негативно сказать на производительности приложения. Для демонстрации возьмем датасет Airport Codes \n",
    "Посчитаем несколько действий. Несмотря на то, что only_ru является общим для всех действий, он пересчитывается при вызове каждого действия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
    "only_ru.show(1, 50, True)\n",
    "\n",
    "only_ru.count()\n",
    "only_ru.collect()\n",
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы следует использовать методы cache, либо persist. Данные методы сохраняют состояние графа после первого действия, и следующие обращаются к нему. Разница между методами заключается в том, что persist позволяет выбрать, куда сохранить данные, а cache использует значение по умолчанию. В текущей версии Spark это StorageLevel.MEMORY_ONLY. Важно помнить, что данный кеш не предназначен для обмена данными между разными Spark приложения - он является внутренним для приложения. После того, как работа с данными окончена, необходимо выполнить unpersist для очистки памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "only_ru = df.filter((col(\"iso_country\") == \"RU\") & (col(\"elevation_ft\") > 1000))\n",
    "only_ru.cache()\n",
    "only_ru.show(1, 50, True)\n",
    "only_ru.count()\n",
    "only_ru.collect()\n",
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()\n",
    "# only_ru.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "only_ru.groupBy(col(\"municipality\")).count().orderBy(col(\"count\").desc()).na.drop(\"any\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Планы выполнения задач\n",
    "Любой job в Spark SQL имеет под собой план выполнения, кототорый генерируется на основе написанно запроса. План запроса содержит операторы, которые затем превращаются в Java код. Поскольку одну и ту же задачу в Spark SQL можно выполнить по-разному, полезно смотреть в планы выполнения, чтобы, например:  \n",
    "\n",
    " - убрать лишние shuffle  \n",
    " - убедиться, чтот тот или иной оператор будет выполнен на уровне источника, а не внутри Spark  \n",
    " - понять, как будет выполнен join  \n",
    " \n",
    "Планы выполнения доступны в двух видах:\n",
    " - метод explain() у DF  \n",
    " - на вкладке SQL в Spark UI  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"type\") == \"small_airport\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним агрегацию и проверим план выполнения. В нем появляется три оператора: 2 HashAggregate и Exchange hashpartitioning.\n",
    "\n",
    "Первый HashAggregate содержит функцию partial_count(1). Это означает, что внутри каждого воркера произойдет подсчет строк по каждому ключу. Затем происходит shuffle по ключу агрегата, после которого выполняется еще один HashAggregate с функцией count(1). Использование двух HashAggregate позволяет сократить количество передаваемых данных по сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col(\"type\") == \"small_airport\").groupBy(col(\"iso_country\")).count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация соединений и группировок\n",
    "При выполнении join двух DF важно следовать рекомендациям:\n",
    "\n",
    "- фильтровать данные до join'а\n",
    "- использовать equ join\n",
    "- если можно путем увеличения количества данных применить equ join вместо non-equ join'а, то делать именно так\n",
    "- всеми силами избегать cross-join'ов\n",
    "- если правый DF помещается в памяти worker'а, использовать broadcast()\n",
    "\n",
    "Виды джойнов\n",
    "\n",
    "- BroadcastHashJoin\n",
    "-- equ join\n",
    "-- broadcast\n",
    "- SortMergeJoin\n",
    "-- equ join\n",
    "-- sortable keys\n",
    "- BroadcastNestedLoopJoin\n",
    "-- non-equ join\n",
    "-- using broadcast\n",
    "- CartesianProduct\n",
    "-- non-equ join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "left = df.select(col(\"type\"), col(\"ident\"), col(\"iso_country\")).alias(\"left\").localCheckpoint()\n",
    "right = df.groupBy(col(\"type\")).count().alias(\"right\").localCheckpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastHashJoin\n",
    "\n",
    "- работает, когда условие - равенство одного или нескольких ключей\n",
    "- работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "- оставляет левый датасет как есть\n",
    "- копирует правый датасет на каждый воркер\n",
    "- составляет hash map из правого датасета, где ключ - кортеж из колонок в условии соединения\n",
    "- итерируется по левому датасета внутри каждой партиции и проверяет наличие ключей в HashMap\n",
    "- может быть автоматически использован, либо явно через broadcast(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "result = left.join(broadcast(right), \"type\", \"inner\")\n",
    "# result = left.join(right, \"type\", \"inner\")\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortMergeJoin\n",
    "+ работает, когда ключи соединения в обоих датасета являются сортируемыми\n",
    "+ репартиционирует оба датасета в 200 партиций по ключу (ключам) соединения\n",
    "+ сортирует партиции каждого из датасетов по ключу (ключам) соединения\n",
    "+ Используя сравнение левого и правого ключей, обходит каждую пару партиций и соединяет строки с одинаковыми ключами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(right, \"type\", \"inner\")\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BroadcastNestedLoopJoin\n",
    "+ работает, когда один из датасетов небольшой и полностью вмещается в память воркера\n",
    "+ оставляет левый датасет как есть\n",
    "+ копирует правый датасет на каждый воркер\n",
    "+ проходится вложенным циклом по каждой партиции левого датасета и копией правого датасета и проверяет условие\n",
    "+ может быть автоматически использован, либо явно через `broadcast(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(broadcast(right), left[\"type\"] != right[\"type\"], \"inner\")\n",
    "\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartesianProduct\n",
    "+ Создает пары из каждой партиции левого датасета с каждой партицией правого датасета, релоцирует каждую пару на один воркер и проверяет условие соединения\n",
    "+ на выходе создает N*M партиций\n",
    "+ работает медленнее остальных и часто приводит к ООМ воркеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "result = left.join(right, left[\"type\"] != right[\"type\"], \"inner\")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снижение количества shuffle\n",
    "\n",
    "В ряде случаев можно уйти от лишних shuffle операций при выполнении соединения. Для этого оба DF должны иметь одинаковое партиционирование - одинаковое количество партиций и ключ партиционирования, совпадающий с ключом соединения.\n",
    "Разница между планами выполнения будет хорошо видна в Spark UI на графе выполнения в Jobs и плане выполнения в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "left = df\n",
    "right = df.groupBy(col(\"type\")).count()\n",
    "joined = left.join(right, \"type\")\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df_rep = df.repartition(200, col(\"type\"))\n",
    "\n",
    "left = df_rep\n",
    "right = df_rep.groupBy(col(\"type\")).count()\n",
    "\n",
    "joined = left.join(right, \"type\")\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизатор запросов Catalyst\n",
    "Catalyst выполняет оптимизацию запросов с целью ускорения их выполнения и применяет следующие методы:\n",
    " + Column projection\n",
    " + Partition pruning\n",
    " + Predicate pushdown\n",
    " + Constant folding\n",
    " \n",
    " Подготовим датасет для демонстрации работы Catalyst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"iso_country\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/airports.parquet\") \n",
    "\n",
    "airports = spark.read.parquet(\"/tmp/airports.parquet\")\n",
    "airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "hdfs dfs -ls /tmp/airports.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "hdfs dfs -ls /tmp/airports.parquet/iso_country=AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.read.parquet(\"/tmp/airports.parquet/iso_country=AD/part-00000-ef581cca-e2a2-4a9e-b75f-d3cf7387ed30.c000.snappy.parquet\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column projection\n",
    "Данный механизм позволяет избегать вычитывания ненужных колонок при работе с источниками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "selected = airports.select(col(\"ident\"))\n",
    "selected.cache()\n",
    "selected.count()\n",
    "selected.unpersist()\n",
    "selected.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "selected = airports\n",
    "selected.cache()\n",
    "selected.count()\n",
    "selected.unpersist()\n",
    "selected.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition pruning\n",
    "Данный механизм позволяет избежать чтения ненужных партиций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "filtered = airports.filter(col(\"iso_country\") == \"RU\")\n",
    "filtered.count()\n",
    "filtered.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicate pushdown\n",
    "Данный механизм позволяет \"протолкнуть\" условия фильтрации данных на уровень datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "filtered = airports.filter(col(\"iso_region\") == \"RU\")\n",
    "filtered.count()\n",
    "filtered.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify casts\n",
    "Данный механизм убирает ненужные `cast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "result = spark.range(10).select(col(\"id\").cast(\"long\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "result = spark.range(10).select(col(\"id\").cast(\"int\").cast(\"long\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant folding\n",
    "Данный механизм сокращает количество констант, используемых в физическом плане"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "result = spark.range(10).select((lit(3) > lit(0)).alias(\"foo\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "result = spark.range(10).select((col(\"id\") >  lit(0)).alias(\"foo\"))\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine filters\n",
    "Данный механизм объединяет фильтры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "result = spark.range(10).filter(col(\"id\") > 0) \\\n",
    "    .filter(col(\"id\") != 5) \\\n",
    "    .filter(col(\"id\") < 10)\n",
    "    \n",
    "result.explain(True)\n"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1820,
    "start_time": "2021-12-02T23:15:57.972Z"
   },
   {
    "duration": 113,
    "start_time": "2021-12-02T23:15:59.794Z"
   },
   {
    "duration": 16,
    "start_time": "2021-12-02T23:16:01.802Z"
   },
   {
    "duration": 9,
    "start_time": "2021-12-02T23:16:04.022Z"
   },
   {
    "duration": 8,
    "start_time": "2021-12-02T23:16:06.131Z"
   },
   {
    "duration": 15,
    "start_time": "2021-12-02T23:16:07.242Z"
   },
   {
    "duration": 398,
    "start_time": "2021-12-02T23:16:08.129Z"
   },
   {
    "duration": 10,
    "start_time": "2021-12-02T23:16:17.583Z"
   },
   {
    "duration": 3,
    "start_time": "2021-12-02T23:16:18.198Z"
   },
   {
    "duration": 9,
    "start_time": "2021-12-02T23:16:18.793Z"
   },
   {
    "duration": 11,
    "start_time": "2021-12-02T23:16:21.930Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:16:22.378Z"
   },
   {
    "duration": 260,
    "start_time": "2021-12-02T23:16:30.000Z"
   },
   {
    "duration": 5,
    "start_time": "2021-12-02T23:16:34.629Z"
   },
   {
    "duration": 2934,
    "start_time": "2021-12-02T23:16:35.275Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:16:41.192Z"
   },
   {
    "duration": 397,
    "start_time": "2021-12-02T23:16:41.783Z"
   },
   {
    "duration": 245,
    "start_time": "2021-12-02T23:16:45.015Z"
   },
   {
    "duration": 4,
    "start_time": "2021-12-02T23:17:25.956Z"
   },
   {
    "duration": 11,
    "start_time": "2021-12-02T23:17:26.132Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:17:32.388Z"
   },
   {
    "duration": 698,
    "start_time": "2021-12-02T23:17:34.958Z"
   },
   {
    "duration": 1414,
    "start_time": "2021-12-02T23:17:38.152Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:17:41.927Z"
   },
   {
    "duration": 1572,
    "start_time": "2021-12-02T23:17:43.848Z"
   },
   {
    "duration": 1156,
    "start_time": "2021-12-02T23:17:48.999Z"
   },
   {
    "duration": 23205,
    "start_time": "2021-12-07T18:42:27.953Z"
   },
   {
    "duration": 2694,
    "start_time": "2021-12-07T18:45:34.103Z"
   },
   {
    "duration": 302,
    "start_time": "2021-12-07T19:15:48.509Z"
   },
   {
    "duration": 21043,
    "start_time": "2021-12-07T19:15:48.813Z"
   },
   {
    "duration": 3,
    "start_time": "2021-12-07T19:49:43.957Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T19:57:11.654Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T20:07:12.050Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T20:14:58.374Z"
   },
   {
    "duration": 86,
    "start_time": "2021-12-10T10:58:16.325Z"
   },
   {
    "duration": 4,
    "start_time": "2021-12-10T11:14:07.509Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "801px",
    "left": "24px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
