{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Union\" data-toc-modified-id=\"Union-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Union</a></span></li><li><span><a href=\"#Union.-Различный-порядок-колонок\" data-toc-modified-id=\"Union.-Различный-порядок-колонок-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Union. Различный порядок колонок</a></span></li><li><span><a href=\"#Safe-union\" data-toc-modified-id=\"Safe-union-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Safe union</a></span></li><li><span><a href=\"#Regexp\" data-toc-modified-id=\"Regexp-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Regexp</a></span></li><li><span><a href=\"#Explode\" data-toc-modified-id=\"Explode-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Explode</a></span></li><li><span><a href=\"#Map-to-columns\" data-toc-modified-id=\"Map-to-columns-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Map to columns</a></span></li><li><span><a href=\"#explode_outer\" data-toc-modified-id=\"explode_outer-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>explode_outer</a></span></li><li><span><a href=\"#Window-functions\" data-toc-modified-id=\"Window-functions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Window functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-df-for-window-examples\" data-toc-modified-id=\"Create-df-for-window-examples-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Create df for window examples</a></span></li><li><span><a href=\"#row_number,-rank,-dense_rank,-percent_rank,-ntile,-cume_dist\" data-toc-modified-id=\"row_number,-rank,-dense_rank,-percent_rank,-ntile,-cume_dist-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>row_number, rank, dense_rank, percent_rank, ntile, cume_dist</a></span></li><li><span><a href=\"#lag-Window-Function\" data-toc-modified-id=\"lag-Window-Function-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>lag Window Function</a></span></li><li><span><a href=\"#Window-Aggregate-Functions\" data-toc-modified-id=\"Window-Aggregate-Functions-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Window Aggregate Functions</a></span></li></ul></li><li><span><a href=\"#UDF\" data-toc-modified-id=\"UDF-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>UDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#Обработка-null\" data-toc-modified-id=\"Обработка-null-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Обработка null</a></span></li><li><span><a href=\"#SQL-register-UDF\" data-toc-modified-id=\"SQL-register-UDF-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>SQL register UDF</a></span></li><li><span><a href=\"#Lambda\" data-toc-modified-id=\"Lambda-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Lambda</a></span></li><li><span><a href=\"#Обработка-массивов\" data-toc-modified-id=\"Обработка-массивов-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Обработка массивов</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "simpleData = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "simpleData2 = [\n",
    "    (\"James\",\"Sales\",\"NY\",90000,34,10000), \n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \n",
    "  ]\n",
    "columns2= [\"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data=simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)\n",
    "\n",
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union. Различный порядок колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "simpleData3 = [\n",
    "    (\"Sales\",\"Jones\",\"NY\",90000,34,10000), \n",
    "    (\"Finance\",\"Maria\",\"CA\",90000,24,23000), \n",
    "    (\"Finance\",\"Jen\",\"NY\",79000,53,15000), \n",
    "  ]\n",
    "columns3= [\"department\", \"employee_name\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df3 = spark.createDataFrame(data = simpleData3, schema = columns3)\n",
    "\n",
    "print(\"df:\")\n",
    "df.show()\n",
    "print(\"df3:\")\n",
    "df3.show()\n",
    "\n",
    "unionDF = df.union(df3)\n",
    "unionDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "df.union(\n",
    "    df3.select(df.columns)\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "habrData\\\n",
    ".select(\"link\")\\\n",
    ".withColumn(\"company_id\", regexp_replace(col(\"link\"), \"(https://habr.com/ru/company/)|(/blog/[0-9]+/)|(https://habr.com/ru/post/[0-9]+/)\", \"\") )\\\n",
    ".show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.subjects).alias(\"exploded\"))\n",
    "df2.show()\n",
    "\n",
    "df2.select(df.name,explode(df2.exploded)).show(truncate=False)\n",
    "\n",
    "df2.select(df.name,explode(df2.exploded)).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "df.schema.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "        ]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name', 'knownLanguages', 'properties'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df3 = df.select(df.name,explode(df.properties))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explode_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,explode_outer(df.knownLanguages)).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,explode_outer(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create df for window examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \n",
    "    (\"Michael\", \"Sales\", 4600),  \n",
    "    (\"Robert\", \"Sales\", 4100),   \n",
    "    (\"Maria\", \"Finance\", 3000),  \n",
    "    (\"James\", \"Sales\", 3000),    \n",
    "    (\"Scott\", \"Finance\", 3300),  \n",
    "    (\"Jen\", \"Finance\", 3900),    \n",
    "    (\"Jeff\", \"Marketing\", 3000), \n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100) \n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, round\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "windowSpec  = Window.partitionBy().orderBy(\"salary\")\n",
    "\n",
    "df\\\n",
    ".withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row_number, rank, dense_rank, percent_rank, ntile, cume_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, percent_rank, ntile, cume_dist, round\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df\\\n",
    ".withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    ".withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    ".withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
    ".withColumn(\"percent_rank\", percent_rank().over(windowSpec)) \\\n",
    ".withColumn(\"ntile\", ntile(3).over(windowSpec)) \\\n",
    ".withColumn(\"cume_dist\", round(cume_dist().over(windowSpec), 2 )) \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lag Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "df.withColumn(\"lag\", lag(\"salary\", 1).over(windowSpec)) \\\n",
    ".withColumn(\"lead\", lead(\"salary\", 2).over(windowSpec)) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    "\n",
    "windowSpec     = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "\n",
    "df.withColumn(\"row\", row_number().over(windowSpec)) \\\n",
    ".withColumn(\"avg_cum\", avg(col(\"salary\")).over(windowSpec)) \\\n",
    ".withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".withColumn(\"sum_cum\", sum(col(\"salary\")).over(windowSpec)) \\\n",
    ".withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    ".show()\n",
    "# .where(col(\"row\") == 1).select(\"department\", \"avg\", \"sum\", \"min\", \"max\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.sql(\"with () as a, select * from \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "habrData = spark.read.option(\"header\", True).csv(\"/datasets/habr_data.csv\")\n",
    " \n",
    "from pyspark.sql.functions import udf, col, round\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def mult(i):\n",
    "    return i * 2\n",
    "\n",
    "multUdf = udf(mult)\n",
    "\n",
    "habrData \\\n",
    ".select(\"rating\") \\\n",
    ".limit(10) \\\n",
    ".withColumn(\"udfString\", multUdf(col(\"rating\"))) \\\n",
    ".withColumn(\"udfInt\", multUdf(col(\"rating\").cast(IntegerType()))) \\\n",
    ".withColumn(\"round\", round(col(\"udfInt\")) ) \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\") \\\n",
    ".selectExpr(\"n + 1\") \\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "def plusOne(i):\n",
    "    return i + 1\n",
    "    \n",
    "plusOneUdf = udf(plusOne)\n",
    "\n",
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\")\\\n",
    ".select(plusOneUdf(col(\"n\")).alias(\"plusOneUdf\"))\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\")\\\n",
    ".selectExpr(\"n + 1 as plusOne\")\\\n",
    ".where(\"plusOne = 2\")\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "spark.createDataFrame(([1], [2], [3]), schema=\"n INT\")\\\n",
    ".withColumn(\"plusOne\", plusOneUdf(col(\"n\")))\\\n",
    ".where(\"plusOne = 2\")\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "habrData = spark.read.option(\"header\", True).csv(\"/datasets/habr_data.csv\").cache()\n",
    " \n",
    "from pyspark.sql.functions import udf, col, when, expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def mult(i):\n",
    "    return i * 2\n",
    "    \n",
    "def mult_nullsafe(i):\n",
    "    if i is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return i * 2\n",
    "    \n",
    "# multUdf = udf(mult) \n",
    "multUdf = udf(mult_nullsafe)\n",
    "\n",
    "# .na.drop(\"all\")\\\n",
    "habrData\\\n",
    ".select(\"rating\")\\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n",
    ".withColumn(\"udfInt\", multUdf(col(\"rating\"))   )\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL register UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "\n",
    "spark.udf.register(\"mult_nullsafe\", mult_nullsafe)\n",
    "\n",
    "habrData \\\n",
    ".where(\"rating is not null\") \\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType())) \\\n",
    ".createOrReplaceTempView(\"habr_data\")\n",
    "\n",
    "spark.sql(\"select rating, mult_nullsafe(rating) plus_one from habr_data\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "\n",
    "# Integer type output\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def square(i):\n",
    "    if i is None:\n",
    "        return 0\n",
    "    else:\n",
    "        return i * i\n",
    "    \n",
    "square_udf_int = udf(lambda z: square(z), IntegerType())\n",
    "\n",
    "habrData\\\n",
    ".select(\"rating\")\\\n",
    ".where(\"rating is not null\")\\\n",
    ".withColumn(\"rating\", col(\"rating\").cast(IntegerType()))\\\n",
    ".withColumn(\"square\", square_udf_int(col(\"rating\"))   )\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка массивов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def square_list_float(x):\n",
    "    return [float(val)**2 for val in x]\n",
    "\n",
    "\n",
    "square_list_float_udf = udf(lambda y: square_list_float(y), ArrayType(FloatType()))\n",
    "\n",
    "\n",
    "cSchema = StructType([StructField(\"int_array\", ArrayType(IntegerType()))])\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[[1, 2]], [[3, 4, 5]], [[6, 7, 8, 9]]], schema=cSchema\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df\\\n",
    ".withColumn(\"square_list_float_udf\", square_list_float_udf(\"int_array\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "print(\"https://arena-hadoop.inno.tech:18088/proxy/\" + sc.applicationId)"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1820,
    "start_time": "2021-12-02T23:15:57.972Z"
   },
   {
    "duration": 113,
    "start_time": "2021-12-02T23:15:59.794Z"
   },
   {
    "duration": 16,
    "start_time": "2021-12-02T23:16:01.802Z"
   },
   {
    "duration": 9,
    "start_time": "2021-12-02T23:16:04.022Z"
   },
   {
    "duration": 8,
    "start_time": "2021-12-02T23:16:06.131Z"
   },
   {
    "duration": 15,
    "start_time": "2021-12-02T23:16:07.242Z"
   },
   {
    "duration": 398,
    "start_time": "2021-12-02T23:16:08.129Z"
   },
   {
    "duration": 10,
    "start_time": "2021-12-02T23:16:17.583Z"
   },
   {
    "duration": 3,
    "start_time": "2021-12-02T23:16:18.198Z"
   },
   {
    "duration": 9,
    "start_time": "2021-12-02T23:16:18.793Z"
   },
   {
    "duration": 11,
    "start_time": "2021-12-02T23:16:21.930Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:16:22.378Z"
   },
   {
    "duration": 260,
    "start_time": "2021-12-02T23:16:30.000Z"
   },
   {
    "duration": 5,
    "start_time": "2021-12-02T23:16:34.629Z"
   },
   {
    "duration": 2934,
    "start_time": "2021-12-02T23:16:35.275Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:16:41.192Z"
   },
   {
    "duration": 397,
    "start_time": "2021-12-02T23:16:41.783Z"
   },
   {
    "duration": 245,
    "start_time": "2021-12-02T23:16:45.015Z"
   },
   {
    "duration": 4,
    "start_time": "2021-12-02T23:17:25.956Z"
   },
   {
    "duration": 11,
    "start_time": "2021-12-02T23:17:26.132Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:17:32.388Z"
   },
   {
    "duration": 698,
    "start_time": "2021-12-02T23:17:34.958Z"
   },
   {
    "duration": 1414,
    "start_time": "2021-12-02T23:17:38.152Z"
   },
   {
    "duration": 7,
    "start_time": "2021-12-02T23:17:41.927Z"
   },
   {
    "duration": 1572,
    "start_time": "2021-12-02T23:17:43.848Z"
   },
   {
    "duration": 1156,
    "start_time": "2021-12-02T23:17:48.999Z"
   },
   {
    "duration": 23205,
    "start_time": "2021-12-07T18:42:27.953Z"
   },
   {
    "duration": 2694,
    "start_time": "2021-12-07T18:45:34.103Z"
   },
   {
    "duration": 302,
    "start_time": "2021-12-07T19:15:48.509Z"
   },
   {
    "duration": 21043,
    "start_time": "2021-12-07T19:15:48.813Z"
   },
   {
    "duration": 3,
    "start_time": "2021-12-07T19:49:43.957Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T19:57:11.654Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T20:07:12.050Z"
   },
   {
    "duration": 2,
    "start_time": "2021-12-07T20:14:58.374Z"
   },
   {
    "duration": 86,
    "start_time": "2021-12-10T10:58:16.325Z"
   },
   {
    "duration": 4,
    "start_time": "2021-12-10T11:14:07.509Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "801px",
    "left": "24px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
